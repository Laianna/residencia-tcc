{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ranking\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VAR_ST = (0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = ['ale_1_1', 'ale_5_1', 'hn_1_1', 'hn_5_1']\n",
    "#arquivos = ['ale_1_1']\n",
    "\n",
    "lista_df_treino = []\n",
    "lista_df_val = []\n",
    "lista_df_teste = []\n",
    "for arquivo in arquivos:\n",
    "\n",
    "    df_treino = pd.read_csv(f\"Dados/Datasets/Treino-Validação/Treino/{arquivo}_treino.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    df_val = pd.read_csv(f\"Dados/Datasets/Treino-Validação/Validação/{arquivo}_validação.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    df_teste = pd.read_csv(f\"Dados/Datasets/Teste/{arquivo}_teste.csv\", dtype = {'ean_1': str, 'ean_2': str})\n",
    "    lista_df_treino.append(df_treino)\n",
    "    lista_df_val.append(df_val)\n",
    "    lista_df_teste.append(df_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando os modelos ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMINHO_MODELO = 'Dados/Resultados/ST/dist-v1/'\n",
    "\n",
    "NOME_MODELO = (\n",
    "               'distiluse-base-multilingual-cased-v1',\n",
    "               'distiluse-base-multilingual-cased-v2',\n",
    "               'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "               'paraphrase-multilingual-mpnet-base-v2'\n",
    "              )\n",
    "\n",
    "APELIDO_MODELO = (\n",
    "                  'dbm-v1',\n",
    "                  'dbm-v2',\n",
    "                  'pml-v2',\n",
    "                  'pmb-v2'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retornar_input_example(titulo_1, titulo_2, label):\n",
    "\n",
    "    return(  InputExample(texts = [ titulo_1, titulo_2 ], label = label)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_embedding(modelo, titulo):\n",
    "\n",
    "    embedding = modelo.encode(titulo)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_st(df_treino, df_val, df_teste, nome_modelo):\n",
    "\n",
    "    num_epocas = 3\n",
    "    \n",
    "    # carregando o modelo\n",
    "    modelo = SentenceTransformer(nome_modelo)\n",
    "\n",
    "    dados_treino = df_treino.apply(lambda row: retornar_input_example(row['titulo_1'], row['titulo_2'], float(row['match'])), axis = 1)\n",
    "    dados_val = df_val.apply(lambda row: retornar_input_example(row['titulo_1'], row['titulo_2'], float(row['match'])), axis = 1)\n",
    "\n",
    "    treino_dataloader = DataLoader(dados_treino, shuffle = True, batch_size = 1)\n",
    "    #treino_perda = losses.CosineSimilarityLoss(model = modelo)\n",
    "    treino_perda = losses.ContrastiveLoss(model = modelo)\n",
    "    #avaliador = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dados_val)\n",
    "    avaliador = evaluation.BinaryClassificationEvaluator.from_input_examples(dados_val)\n",
    "\n",
    "    # fine-tune do modelo\n",
    "    modelo.fit(\n",
    "               train_objectives = [(treino_dataloader, treino_perda)],\n",
    "               epochs = num_epocas,\n",
    "               evaluator = avaliador\n",
    "              )\n",
    "    \n",
    "    # calculando os embeddings\n",
    "    embedding_1 = df_teste.apply(lambda row: calcular_embedding(modelo, row['titulo_1']), axis = 1)\n",
    "    embedding_2 = df_teste.apply(lambda row: calcular_embedding(modelo, row['titulo_2']), axis = 1)\n",
    "\n",
    "    similaridade = []\n",
    "    for i in range(len(embedding_1)):\n",
    "        # Calculando a matriz de similaridade. Quanto maior o score maior a similaridade.\n",
    "        similaridade.append(np.inner(embedding_1[i], embedding_2[i]))\n",
    "\n",
    "    return similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_y_pred(limite, similaridade):\n",
    "\n",
    "    y_pred = [1 if num >= limite else 0 for num in similaridade]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def montar_df_resultado(y_teste, y_pred, df_teste, nome):\n",
    "    df_y = pd.DataFrame(\n",
    "                        list(zip(\n",
    "                                 y_teste, y_pred,\n",
    "                                 df_teste[\"categoria\"].to_list(),\n",
    "                                 df_teste[\"titulo_1\"].to_list(),\n",
    "                                 df_teste[\"titulo_2\"].to_list()\n",
    "                                )\n",
    "                       ), columns = ['match', 'pred', 'categoria', 'titulo_1', 'titulo_2'])\n",
    "\n",
    "    return df_y\n",
    "\n",
    "\n",
    "def salvar_similaridade(y_teste, similaridade, df_teste, nome, apelido):\n",
    "\n",
    "    df_y = montar_df_resultado(y_teste, similaridade, df_teste, nome)\n",
    "    df_y.to_csv(f'Dados/Resultados/ST-{apelido}/{nome}_similaridade.csv', index = False)\n",
    "\n",
    "\n",
    "def salvar_y_pred(y_teste, y_pred, df_teste, nome, apelido, limite):\n",
    "\n",
    "    df_y = montar_df_resultado(y_teste, y_pred, df_teste, nome)\n",
    "    df_y.to_csv(f'Dados/Resultados/ST-{apelido}/ST_{limite}/Resultado/{nome}_y.csv', index = False)\n",
    "\n",
    "\n",
    "def salvar_relatorio(y_teste, y_pred, nome, apelido, tempo, limite):\n",
    "\n",
    "    relatorio = classification_report(y_teste, y_pred, output_dict = True)\n",
    "    df_relatorio = pd.DataFrame(relatorio).transpose()\n",
    "    df_relatorio['modelo'] = nome\n",
    "    df_relatorio['tempo'] = tempo\n",
    "\n",
    "    df_relatorio.to_csv(f'Dados/Resultados/ST-{apelido}/ST_{limite}/Relatório/{nome}_relatório.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_st = True\n",
    "if flag_st == True:\n",
    "\n",
    "    numero_modelo = 0\n",
    "    nome_modelo = NOME_MODELO[numero_modelo]\n",
    "    apelido_modelo = APELIDO_MODELO[numero_modelo]\n",
    "    tam = 5\n",
    "\n",
    "    lista_df_resultado = []\n",
    "    for nome, df_treino, df_val, df_teste in zip(arquivos, lista_df_treino, lista_df_val, lista_df_teste):\n",
    "\n",
    "        y_teste = df_teste[\"match\"].to_list()\n",
    "        \n",
    "        inicio_tempo = time.time()\n",
    "        #y_teste = y_teste[:tam]\n",
    "        #similaridade = pipeline_st(df_treino[:tam], df_val[:tam], df_teste[:tam], nome_modelo)\n",
    "        similaridade = pipeline_st(df_treino, df_val, df_teste, nome_modelo)\n",
    "        final_tempo = time.time()\n",
    "        tempo = final_tempo - inicio_tempo\n",
    "\n",
    "        salvar_similaridade(y_teste, similaridade, df_teste, nome, apelido_modelo)\n",
    "\n",
    "        for limite in VAR_ST:\n",
    "\n",
    "            y_pred = calcular_y_pred(limite, similaridade)\n",
    "\n",
    "            salvar_y_pred(y_teste, y_pred, df_teste, nome, apelido_modelo, ranking.remove_pontuacao(str(limite)) )\n",
    "            salvar_relatorio(y_teste, y_pred, nome, apelido_modelo, tempo, ranking.remove_pontuacao(str(limite)) )\n",
    "        \n",
    "        '''y_pred = calcular_y_pred(similaridade, 0.5)\n",
    "\n",
    "        report = classification_report(y_teste[:tam], y_pred, output_dict = True)\n",
    "        #report = classification_report(y_teste, y_pred, output_dict = True)\n",
    "        df_resultado = pd.DataFrame(report).transpose()\n",
    "        df_resultado['modelo'] = nome\n",
    "        df_resultado['tempo'] = tempo\n",
    "\n",
    "        df_resultado.to_csv(f'Dados/Resultados/ST-{apelido_modelo}/{nome}_resultado.csv', index = True)\n",
    "\n",
    "        df_y = pd.DataFrame(list(zip(y_teste[:tam], similaridade)), columns = ['match', 'pred'])\n",
    "        df_y.to_csv(f'Dados/Resultados/ST-{apelido_modelo}/y/{nome}_y.csv', index = False)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4889b298c2b41a0f37c917b2884deb5a3a36b040cfb99d0e8c1edb9e5e6fadf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
