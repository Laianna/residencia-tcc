{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame( titulo_bow , columns = cv.get_feature_names() )\n",
    "\n",
    "feature_vector_1 = df_bow.loc[0, :]\n",
    "feature_vector_2 = df_bow.loc[1, :]\n",
    "\n",
    "calcular_dis_cos(feature_vector_1, feature_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_1, valores_1 = calcular_k_maiores(resultado, 1)\n",
    "indices_10, valores_10 = calcular_k_maiores(resultado, 10)\n",
    "indices_50, valores_50 = calcular_k_maiores(resultado, 50)\n",
    "\n",
    "for indices in [indices_1, indices_10, indices_50]:\n",
    "\n",
    "    # criando uma coluna nova no df\n",
    "    df_pp[f'k-{len(indices[0])}'] = 0\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "\n",
    "        for j in range(len(indices[0])):\n",
    "            \n",
    "            if df_pp['ean'].loc[i] == df_pp['ean'].loc[indices[i][j]]:\n",
    "\n",
    "                df_pp[f'k-{len(indices[0])}'].loc[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_k_maiores(res, k):\n",
    "\n",
    "    indices = []\n",
    "    valores = []\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        \n",
    "        ind = res[i].argsort()[-k:]\n",
    "        val = res[i][res[i].argsort()[-k:]]\n",
    "\n",
    "        indices.append( list(reversed(ind)) )\n",
    "        valores.append( list(reversed(val)) )\n",
    "\n",
    "    return indices, valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_nao_pares_hn(df, n_matches, ean_repetido, indices_resultado, colunas = COLUNAS):\n",
    "\n",
    "    lista_3 = []\n",
    "    df_nao_pares_hn = pd.DataFrame(columns = colunas)\n",
    "\n",
    "    limite = ceil(n_matches/len(ean_repetido))\n",
    "    print(f\"limite: {limite}\")\n",
    "\n",
    "    for i in range(len(ean_repetido)):\n",
    "\n",
    "        lista_1 = []\n",
    "        lista_2 = []\n",
    "\n",
    "        ean = ean_repetido[i]\n",
    "        indice_df = df[df['ean'] == ean].head(1).index.values[0] # me da o 1º indice do dataframe onde tem o EAN repetido\n",
    "        lista_1.append(indice_df)\n",
    "\n",
    "        flag_encontrou = False\n",
    "        j = 0\n",
    "        cont = 0\n",
    "        while (flag_encontrou == False):\n",
    "            \n",
    "            indice_rank = indices_resultado[indice_df][j]\n",
    "\n",
    "            if df['ean'].loc[indice_df] != df['ean'].loc[indice_rank]:\n",
    "\n",
    "\n",
    "                lista_2.append(indice_rank)\n",
    "\n",
    "                cont += 1\n",
    "\n",
    "                if cont > limite:\n",
    "                    flag_encontrou = True\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        lista_3.append([ean, lista_1, lista_2])\n",
    "\n",
    "    return lista_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some texts of different lengths.\n",
    "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n",
    "italian_sentences = [\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\n",
    "japanese_sentences = [\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]\n",
    "portuguese_sentences = [\"Olá amigo, tudo bem?\"]\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "# Compute embeddings.\n",
    "en_result = embed(english_sentences)\n",
    "it_result = embed(italian_sentences)\n",
    "ja_result = embed(japanese_sentences)\n",
    "br_result = embed(portuguese_sentences)\n",
    "\n",
    "# Compute similarity matrix. Higher score indicates greater similarity.\n",
    "similarity_matrix_it = np.inner(en_result, it_result)\n",
    "similarity_matrix_ja = np.inner(en_result, ja_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Similarity\n",
    "\n",
    "These models find semantically similar sentences within one language or across languages:\n",
    "\n",
    "distiluse-base-multilingual-cased-v1: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. Supports 15 languages: Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish.\n",
    "\n",
    "distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. This version supports 50+ languages, but performs a bit weaker than the v1 model.\n",
    "\n",
    "paraphrase-multilingual-MiniLM-L12-v2 - Multilingual version of paraphrase-MiniLM-L12-v2, trained on parallel data for 50+ languages.\n",
    "\n",
    "paraphrase-multilingual-mpnet-base-v2 - Multilingual version of paraphrase-mpnet-base-v2, trained on parallel data for 50+ languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.6691]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "query_embedding = model.encode('How many people live in London?')\n",
    "\n",
    "#The passages are encoded as [ [title1, text1], [title2, text2], ...]\n",
    "passage_embedding = model.encode([['London', 'London has 9,787,426 inhabitants at the 2011 census.']])\n",
    "\n",
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo o fit do SentenceTransformer usando mais parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_st(df_treino):\n",
    "\n",
    "    num_epocas = 3\n",
    "    \n",
    "    # carregando o modelo\n",
    "    modelo = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "    dados_treino = df_treino.apply(lambda row: retornar_input_example(row['titulo_1'], row['titulo_2'], float(row['match'])), axis = 1)\n",
    "\n",
    "    treino_dataloader = DataLoader(dados_treino, shuffle = True, batch_size = 1)\n",
    "    treino_perda = losses.CosineSimilarityLoss(model = modelo)\n",
    "\n",
    "    avaliador = evaluation.EmbeddingSimilarityEvaluator(df_treino['titulo_1'], df_treino['titulo_2'], df_treino[\"match\"])\n",
    "\n",
    "    # fine-tune do mocelo\n",
    "    modelo.fit(\n",
    "               train_objectives=[(treino_dataloader, treino_perda)],\n",
    "               evaluator = avaliador,\n",
    "               epochs = num_epocas,\n",
    "               evaluation_steps = 1000,\n",
    "               warmup_steps = 100,\n",
    "               output_path = CAMINHO_MODELO\n",
    "              )\n",
    "    \n",
    "    #Predict test data\n",
    "    '''label_pred = get_test_prediction(model, ds_test)\n",
    "\n",
    "    return (name, bert_history, y_test, label_pred)'''\n",
    "\n",
    "    # metrics = calc_metrics(y_test, result, name)\n",
    "    # return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserindo a coluna \"categoria\" nos pares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomes = [\"celulares\", \"notebooks\", \"geladeiras\", \"fogoes\", \"tvs\"]\n",
    "df = pd.read_csv(f\"Dados/Pares/Sem Categoria/pares_{lista_nomes[0]}.csv\")\n",
    "df.insert(loc = 12, column = 'categoria', value = lista_nomes[0])\n",
    "df.to_csv(f\"Dados/Pares/pares_{lista_nomes[0]}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4889b298c2b41a0f37c917b2884deb5a3a36b040cfb99d0e8c1edb9e5e6fadf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
